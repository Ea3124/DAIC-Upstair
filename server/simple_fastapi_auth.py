from __future__ import annotations
"""
PNU Scholarship Parser API â€“ v1.1
--------------------------------
* ê°œì„  ì‚¬í•­
  1. **ì¤‘ë³µ íŒŒì‹± ë°©ì§€** â€“ ì²¨ë¶€íŒŒì¼ì„ SHAâ€‘256 í•´ì‹œë¡œ ì‹ë³„í•´ ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œëŠ” ê±´ë„ˆëœ€.
  2. **FAISS ì¸ë±ìŠ¤ ì˜ì†/ì¦ë¶„** â€“ ì¸ë±ìŠ¤ ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ë©´ ë¡œë“œ í›„ ìƒˆ ì²­í¬ë§Œ ì¶”ê°€.
  3. **ì¥í•™ê¸ˆ ì¡°ê±´ í‚¤ì›Œë“œ ë§¤ì¹­** â€“ ê°„ë‹¨í•œ ë£° ê¸°ë°˜ ìŠ¤ìºë‹(ì•ŒëŒìš©) + ë¡œê¹….
  4. **ë¡œê¹… ê°•í™”** â€“ ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™©ê³¼ ìŠ¤í‚µ ì‚¬ìœ ê°€ ì½˜ì†”ì— ì¶œë ¥.

"""
from fastapi import APIRouter
# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€
import hashlib
import json
import logging
import mimetypes
import os
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime
from typing import Dict, List
from urllib.parse import urljoin

from db.db import SessionLocal, Document  # DB ì„¸ì…˜ ë° ëª¨ë¸


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, BackgroundTasks
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_upstage import UpstageEmbeddings
from langchain_community.vectorstores import FAISS
from requests.adapters import HTTPAdapter
from requests.exceptions import HTTPError, ReadTimeout
from urllib3.util import Retry
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
if not UPSTAGE_API_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ UPSTAGE_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¡œê¹… â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger("pnu_parser")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(levelname)s | %(message)s"))
logger.addHandler(handler)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI â”€â”€â”€â”€â”€â”€â”€â”€â”€
scholarship_router = APIRouter()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://cse.pusan.ac.kr"
LIST_URL = f"{BASE_URL}/bbs/cse/2605/artclList.do"
SUPPORTED_EXTENSIONS = (".pdf", ".hwp", ".hwpx", ".docx", ".ppt", ".pptx")
KEYWORD = "ì¥í•™"
CATEGORY_SEQ = "4229"
MAX_PAGES = 1
MAX_NOTICES = 8  # ì²˜ë¦¬í•  ê³µì§€ ê°œìˆ˜ í™•ëŒ€

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¥í•™ê¸ˆ ë£° (ì˜ˆì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCHOLARSHIP_RULES = {
    "êµ­ê°€ê·¼ë¡œ": ["êµ­ê°€ê·¼ë¡œ", "ê·¼ë¡œì¥í•™"],
    "í‘¸ë¥¸ë“±ëŒ€": ["í‘¸ë¥¸ë“±ëŒ€"],
    "ì§€ì—­ì¸ì¬": ["ì§€ì—­ì¸ì¬", "íŠ¹ë³„"]
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° êµ¬ì¡° â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class AttachmentDoc:
    id: int
    file_name: str
    content_html: str
    content_text: str
    matched_rules: List[str]

parsed_notices: Dict[int, Dict] = {}
next_notice_id: int = 1
next_attach_id: int = 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Embedding & FAISS â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings = UpstageEmbeddings(
    model="solar-embedding-1-large",
    upstage_api_key=UPSTAGE_API_KEY,
)
vector_store: FAISS | None = None
VECTOR_DIR = Path("./faiss_index")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•´ì‹œ ì €ì¥ì†Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
HASH_FILE = Path("known_hashes.json")
if HASH_FILE.exists():
    known_hashes = set(json.loads(HASH_FILE.read_text()))
else:
    known_hashes: set[str] = set()

def save_known_hashes():
    HASH_FILE.write_text(json.dumps(list(known_hashes), ensure_ascii=False, indent=2))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Session & Retry â”€â”€â”€â”€â”€â”€â”€â”€â”€
retry_policy = Retry(total=2, backoff_factor=1.5, allowed_methods={"POST"}, status_forcelist=[502, 503, 504])
session = requests.Session()
session.mount("https://", HTTPAdapter(max_retries=retry_policy))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sha256_bytes(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()


def guess_mime(fname: str) -> str:
    mime, _ = mimetypes.guess_type(fname)
    if mime:
        return mime
    return {".hwp": "application/x-hwp", ".hwpx": "application/x-hwp"}.get(Path(fname).suffix.lower(), "application/octet-stream")


def call_upstage(file_name: str, file_bytes: bytes) -> dict:
    """Upstage Document-Parse í˜¸ì¶œ í›„ JSON ë°˜í™˜"""
    api_url = "https://api.upstage.ai/v1/document-digitization"
    headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}
    files = {"document": (file_name, file_bytes, guess_mime(file_name))}
    data = {"model": "document-parse"}

    try:
        resp = session.post(api_url, headers=headers, files=files, data=data, timeout=(10, 180))
        resp.raise_for_status()
        return resp.json()
    except ReadTimeout:
        logger.warning(f"â° ReadTimeout â–¶ {file_name}")
        raise
    except HTTPError as e:
        logger.error(f"HTTPError â–¶ {file_name} â€“ {e}")
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‚¤ì›Œë“œ ë§¤ì¹­ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def match_rules(text: str) -> List[str]:
    matched = []
    lower = text.lower()
    for rule_name, kws in SCHOLARSHIP_RULES.items():
        if all(kw.lower() in lower for kw in kws):
            matched.append(rule_name)
    return matched

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def crawl_and_parse(keyword: str = KEYWORD, max_pages: int = MAX_PAGES, max_notices: int = MAX_NOTICES) -> None:
    """ì¥í•™ ì¹´í…Œê³ ë¦¬ + ì œëª© ê²€ìƒ‰ í¬ë¡¤ëŸ¬"""
    global next_notice_id, next_attach_id

    headers = {"User-Agent": "Mozilla/5.0"}
    logger.info("[START] í¬ë¡¤ë§ ì‹œì‘")

    page = 1
    while page <= max_pages and len(parsed_notices) < max_notices:
        payload = {"srchColumn": "sj", "srchWrd": keyword, "bbsClSeq": CATEGORY_SEQ, "page": str(page), "isViewMine": "false"}
        resp = session.post(LIST_URL, headers=headers, data=payload, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        articles = soup.select("td._artclTdTitle a.artclLinkView")
        if not articles:
            break
        logger.info(f"[INFO] page {page} - ê¸€ {len(articles)}ê°œ")

        for a in articles:
            if len(parsed_notices) >= max_notices:
                break

            row = a.find_parent("tr")
            if row and any(cls.startswith("headline") for cls in (row.get("class") or [])):
                logger.info(f"[SKIP] ê³ ì •ê³µì§€: {a.get_text(strip=True)}")
                continue

            notice_title = a.get_text(strip=True)
            detail_url = urljoin(BASE_URL, a["href"])
            logger.info(f"[INFO] ì²˜ë¦¬ ì¤‘(ê³µì§€): {notice_title}")

            notice_id = next_notice_id
            parsed_notices[notice_id] = {"title": notice_title, "url": detail_url, "attachments": []}

            next_notice_id += 1

            try:
                detail_resp = session.get(detail_url, headers=headers, timeout=15)
                detail_resp.raise_for_status()
                detail_soup = BeautifulSoup(detail_resp.text, "html.parser")
            except Exception as e:
                logger.error(f"[ERROR] ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                continue

            attachments = detail_soup.select('dl.artclForm dd.artclInsert li a[href*="/download.do"]')
            if not attachments:
                logger.info("[SKIP] ì²¨ë¶€ ì—†ìŒ")
                continue

            for file_link in attachments:
                file_name = file_link.get_text(strip=True)
                if not file_name.lower().endswith(SUPPORTED_EXTENSIONS):
                    logger.warning(f"[SKIP] ë¯¸ì§€ì› í¬ë§·: {file_name}")
                    continue

                file_url = urljoin(detail_url, file_link["href"])
                try:
                    file_resp = session.get(file_url, headers=headers, timeout=30)
                    file_resp.raise_for_status()
                except Exception as e:
                    logger.error(f"[ERROR] íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_name} â€“ {e}")
                    continue

                f_hash = sha256_bytes(file_resp.content)
                if f_hash in known_hashes:
                    logger.info(f"[SKIP] ì¤‘ë³µ íŒŒì¼: {file_name}")
                    continue

                # â”€â”€ Upstage ë³€í™˜ ìš”ì²­ â”€â”€
                logger.info(f"ğŸ“¤ Upstage ìš”ì²­ ì‹œì‘ â–¶ {file_name}")
                try:
                    result_json = call_upstage(file_name, file_resp.content)
                except (ReadTimeout, HTTPError):
                    continue

                html_segments = [elem["content"]["html"] for elem in result_json.get("elements", []) if "content" in elem and "html" in elem["content"]]
                full_html = "\n".join(html_segments) or "<p>(ë¹ˆ ë¬¸ì„œ)</p>"

                text_segments = [BeautifulSoup(h, "html.parser").get_text(" ", strip=True) for h in html_segments]
                full_text = "\n".join(filter(None, text_segments)) or "(ë¹ˆ ë¬¸ì„œ)"

                # íŒŒì¼ì—ì„œ ì¥í•™ê¸ˆ ì¡°ê±´ ì¶”ì¶œ
                client = OpenAI(
                    api_key=UPSTAGE_API_KEY,
                    base_url="https://api.upstage.ai/v1"
                )
                prompt = f"""Extract scholarship conditions from the following text:
                {full_text}
                Return the conditions in csv format including min_gpa,start_date,end_date,grade,status.
                min_gpa is a float number between 0 and 4.5 inclusive.
                min_gpa is the minimum GPA required for the scholarship.
                start_date and end_date are date strings in YYYY-MM-DD format.
                start_date is the start date of application period of the scholarship.
                end_date is the end date of application period of the scholarship.
                grade is an integer between 1 and 4.
                grade is the attending year of the student.
                status is a string that can be ì¬í•™ or íœ´í•™.
                if min_gpa is not found, return an empty string.
                if start date is not found, return an empty string.
                if end date is not found, return an empty string.
                if grade is not found, return an empty string.
                if status is not found, return an empty string.
                the following is an example of the response:
                min_gpa: 2.5, start_date: 2024-03-01, end_date: 2024-08-31, grade: 1, status: ì¬í•™
                Do not include any other text in the response.
                """
                response = client.chat.completions.create(
                    model="solar-pro2-preview",
                    messages=[{"role": "user", "content": prompt}]
                )
                conditions = response.choices[0].message.content
     
                if conditions:
                    try:
                        import re

                        pattern_gpa = r"min_gpa:\s*([\d.]+)"
                        match = re.search(pattern_gpa, conditions)

                        try:
                            gpa = match.group(1)
                        except:
                            gpa = None

                        pattern_start_date = r"start_date:\s*([\d-]+)"
                        match = re.search(pattern_start_date, conditions)

                        try:
                            start_date = match.group(1)
                        except:
                            start_date = None

                        pattern_end_date = r"end_date:\s*([\d-]+)"
                        match = re.search(pattern_end_date, conditions)

                        try:
                            end_date = match.group(1)
                        except:
                            end_date = None

                        pattern_grade = r"grade:\s*(\d+)"
                        match = re.search(pattern_grade, conditions)

                        try:
                            grade = match.group(1)
                        except:
                            grade = None

                        pattern_status = r"status:\s*(\S+)"
                        match = re.search(pattern_status, conditions)

                        try:
                            status = match.group(1)
                        except:
                            status = None

                        
                    except Exception as e:
                        logger.error(f"âŒ ì¡°ê±´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue

                matched = match_rules(full_text)
                if matched:
                    logger.info(f"ğŸ”” ì•Œë¦¼ ë§¤ì¹­: {matched} â€“ {file_name}")
                else:
                    logger.info(f"âšª ì¡°ê±´ ë¯¸ì¼ì¹˜: {file_name}")

                parsed_notices[notice_id]["attachments"].append(
                    AttachmentDoc(id=next_attach_id, file_name=file_name, content_html=full_html, content_text=full_text, matched_rules=matched)
                )
                # DBì— ë¬¸ì„œ ì €ì¥
                try:
                    db = SessionLocal()
                    new_doc = Document(
                        title=notice_title,
                        link=detail_url,
                        content=full_text,
                        gpa=float(gpa) if gpa else None,
                        start_date=datetime.strptime(start_date, "%Y-%m-%d") if start_date else None,
                        end_date=datetime.strptime(end_date, "%Y-%m-%d") if end_date else None,
                        status=status if status else None,
                        grade=int(grade) if grade else None,
                    )
                    db.add(new_doc)
                    db.commit()
                    logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {notice_title}")
                except Exception as e:
                    try:
                        db.rollback()
                        db = SessionLocal()
                        new_doc = Document(
                            title=notice_title,
                            link=detail_url,
                            content=full_text,
                            gpa=None,
                            start_date=None,
                            end_date=None,
                            status=None,
                            grade=None,
                        )
                        db.add(new_doc)
                        db.commit()
                        logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {notice_title}")
                    except Exception as e:
                        logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
                finally:
                    db.close()
                next_attach_id += 1
                known_hashes.add(f_hash)
        page += 1

    save_known_hashes()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ FAISS ì¸ë±ì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_faiss_index() -> None:
    """ìƒˆ ë¬¸ì„œë§Œ ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ê±°ë‚˜, ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±"""
    global vector_store

    if not parsed_notices:
        logger.warning("ë¹Œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)

    # 1ï¸âƒ£ ìƒˆë¡œ ìˆ˜ì§‘ëœ ëª¨ë“  í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜ì§‘
    new_texts: List[str] = []
    new_metas: List[dict] = []

    for notice in parsed_notices.values():
        for attach in notice["attachments"]:
            for chunk in splitter.split_text(attach.content_html):
                new_texts.append(chunk)
                new_metas.append({
                    "notice_title": notice["title"],
                    "attachment_id": attach.id,
                    "file_name": attach.file_name,
                    "url": notice.get("url"), 
                })  

    if not new_texts:
        logger.info("[FAISS] ì¶”ê°€í•  ì²­í¬ ì—†ìŒ â€“ ì €ì¥ ìŠ¤í‚µ")
        return

    # 2ï¸âƒ£ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ or ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
    if VECTOR_DIR.exists():
        logger.info("[FAISS] ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ")
        vector_store = FAISS.load_local(str(VECTOR_DIR), embeddings, allow_dangerous_deserialization=True,)
        vector_store.add_texts(new_texts, metadatas=new_metas)
    else:
        logger.info("[FAISS] ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")
        vector_store = FAISS.from_texts(texts=new_texts, embedding=embeddings, metadatas=new_metas)

    # 3ï¸âƒ£ ì €ì¥
    vector_store.save_local(str(VECTOR_DIR))
    logger.info(f"[âœ… FAISS] {len(new_texts)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ API â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€

@scholarship_router.post("/notices/refresh")
def refresh_notices(background_tasks: BackgroundTasks, keyword: str = KEYWORD):
    parsed_notices.clear()
    global next_notice_id, next_attach_id
    next_notice_id = 1
    next_attach_id = 1
    try:
        crawl_and_parse(keyword)
        # ì¸ë±ì‹±ì€ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰í•´ ì‘ë‹µ ì†ë„ í–¥ìƒ
        background_tasks.add_task(build_faiss_index)
        return {"status": "success", "notices": len(parsed_notices)}
    except Exception as e:
        raise HTTPException(500, str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ (ì˜µì…˜) ì„œë²„ ê¸°ë™ ì‹œ ìë™ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€
# crawl_and_parse()
