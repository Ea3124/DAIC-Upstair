from __future__ import annotations

"""
PNU Scholarship Parser API  (ê³µì§€ 1ê°œ â€“ ì²¨ë¶€ Nê°œ êµ¬ì¡°)
---------------------------------------------------
ì‹¤í–‰ ì˜ˆì‹œ
$ uvicorn scholarship_parser:app --reload
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import logging
import mimetypes
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List
from urllib.parse import urljoin

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_upstage import UpstageEmbeddings
from langchain_community.vectorstores import FAISS
from requests.adapters import HTTPAdapter               # â˜…
from requests.exceptions import HTTPError, ReadTimeout  # â˜…
from urllib3.util import Retry                          # â˜…

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
if not UPSTAGE_API_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ UPSTAGE_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¡œê¹… ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger(__name__)     # â˜… ëª¨ë“ˆ ë¡œê±° ì‚¬ìš©
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(levelname)s | %(message)s"))
logger.addHandler(handler)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(title="PNU Scholarship Parser API")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://cse.pusan.ac.kr"
LIST_URL = f"{BASE_URL}/bbs/cse/2605/artclList.do"
SUPPORTED_EXTENSIONS = (
    ".pdf",
    ".hwp",
    ".hwpx",
    ".docx",
    ".ppt",
    ".pptx",
)
KEYWORD = "ì¥í•™"
CATEGORY_SEQ = "4229"
MAX_PAGES = 1
MAX_NOTICES = 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¸ë©”ëª¨ë¦¬ ë°ì´í„° êµ¬ì¡° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class AttachmentDoc:
    id: int
    file_name: str
    content_html: str
    content_text: str

parsed_notices: Dict[int, Dict] = {}
next_notice_id: int = 1
next_attach_id: int = 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Embedding & FAISS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings = UpstageEmbeddings(
    model="solar-embedding-1-large",
    upstage_api_key=UPSTAGE_API_KEY,
)
vector_store: FAISS | None = None
VECTOR_DIR = Path("./faiss_index")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Session & Retry ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
retry_policy = Retry(                              # â˜…
    total=2,
    backoff_factor=1.5,
    allowed_methods={"POST"},
    status_forcelist=[502, 503, 504],
)
session = requests.Session()                       # â˜…
session.mount("https://", HTTPAdapter(max_retries=retry_policy))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def guess_mime(fname: str) -> str:
    mime, _ = mimetypes.guess_type(fname)
    if mime:
        return mime
    ext = Path(fname).suffix.lower()
    return {
        ".hwp": "application/x-hwp",
        ".hwpx": "application/x-hwp",
    }.get(ext, "application/octet-stream")


def call_upstage(file_name: str, file_bytes: bytes) -> dict:
    """Upstage Document-Parse í˜¸ì¶œ í›„ JSON ë°˜í™˜ (ReadTimeout ì²˜ë¦¬)"""
    api_url = "https://api.upstage.ai/v1/document-digitization"
    headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}
    files = {"document": (file_name, file_bytes, guess_mime(file_name))}
    data = {"model": "document-parse"}

    try:
        resp = session.post(
            api_url,
            headers=headers,
            files=files,
            data=data,
            timeout=(10, 180),       # â˜… (connect 10s, read 180s)
        )
        resp.raise_for_status()
    except ReadTimeout as e:          # â˜…
        logger.warning(f"â° ReadTimeout â–¶ {file_name} â€“ {e}")
        raise
    except HTTPError as e:
        logger.error(f"HTTPError â–¶ {file_name} â€“ {e}")
        raise
    return resp.json()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl_and_parse(
    keyword: str = KEYWORD,
    max_pages: int = MAX_PAGES,
    max_notices: int = MAX_NOTICES,
) -> None:
    """ì¥í•™ ì¹´í…Œê³ ë¦¬ + ì œëª© ê²€ìƒ‰ í¬ë¡¤ëŸ¬"""
    global next_notice_id, next_attach_id

    headers = {"User-Agent": "Mozilla/5.0"}

    logger.info("[START] í¬ë¡¤ë§ ì‹œì‘")
    page = 1
    while page <= max_pages and len(parsed_notices) < max_notices:
        payload = {
            "srchColumn": "sj",
            "srchWrd": keyword,
            "bbsClSeq": CATEGORY_SEQ,
            "page": str(page),
            "isViewMine": "false",
        }

        resp = session.post(LIST_URL, headers=headers, data=payload, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        articles = soup.select("td._artclTdTitle a.artclLinkView")
        if not articles:
            break

        logger.info(f"[INFO] page {page} - ê¸€ {len(articles)}ê°œ")
        for a in articles:
            if len(parsed_notices) >= max_notices:
                break

            row = a.find_parent("tr")
            if row and any(cls.startswith("headline") for cls in (row.get("class") or [])):
                logger.info(f"[SKIP] ê³ ì •ê³µì§€: {a.get_text(strip=True)}")
                continue

            notice_title = a.get_text(strip=True)
            detail_url = urljoin(BASE_URL, a["href"])
            logger.info(f"[INFO] ì²˜ë¦¬ ì¤‘(ê³µì§€): {notice_title}")

            notice_id = next_notice_id
            parsed_notices[notice_id] = {"title": notice_title, "attachments": []}
            next_notice_id += 1

            try:
                detail_resp = session.get(detail_url, headers=headers, timeout=15)
                detail_resp.raise_for_status()
                detail_soup = BeautifulSoup(detail_resp.text, "html.parser")
            except Exception as e:
                logger.error(f"[ERROR] ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                continue

            attachments = detail_soup.select(
                'dl.artclForm dd.artclInsert li a[href*="/download.do"]'
            )
            if not attachments:
                logger.info("[SKIP] ì²¨ë¶€ ì—†ìŒ")
                continue

            for file_link in attachments:
                file_name = file_link.get_text(strip=True)
                if not file_name.lower().endswith(SUPPORTED_EXTENSIONS):
                    logger.warning(f"[SKIP] ë¯¸ì§€ì› í¬ë§·: {file_name}")
                    continue

                file_url = urljoin(detail_url, file_link["href"])
                try:
                    file_resp = session.get(file_url, headers=headers, timeout=30)
                    file_resp.raise_for_status()
                except Exception as e:
                    logger.error(f"[ERROR] íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_name} â€“ {e}")
                    continue

                # â”€â”€ Upstage ë³€í™˜ ìš”ì²­ â”€â”€
                logger.info(f"ğŸ“¤ Upstage ìš”ì²­ ì‹œì‘ â–¶ {file_name}")          # â˜…
                try:
                    result_json = call_upstage(file_name, file_resp.content)
                except ReadTimeout:
                    logger.error(f"[SKIP] ë³€í™˜ ì§€ì—° â–¶ {file_name}")        # â˜…
                    continue
                except HTTPError as e:
                    logger.error(f"[ERROR] {file_name} ë³€í™˜ ì‹¤íŒ¨: {e}")
                    continue

                html_segments = [
                    elem["content"]["html"]
                    for elem in result_json.get("elements", [])
                    if "content" in elem
                ]
                full_html = "\n".join(html_segments) or "<p>(ë¹ˆ ë¬¸ì„œ)</p>"

                text_segments = []

                for html in html_segments:
                    soup = BeautifulSoup(html, "html.parser")
                    plain_text = soup.get_text(" ", strip=True)
                    if plain_text:
                        text_segments.append(plain_text)

                full_text = "\n".join(text_segments) or "(ë¹ˆ ë¬¸ì„œ)"

                parsed_notices[notice_id]["attachments"].append(
                    AttachmentDoc(
                        id=next_attach_id,
                        file_name=file_name,
                        content_html=full_html,
                        content_text=full_text,
                    )
                )
                logger.info(
                    f"[âœ… ì €ì¥] notice {notice_id}, attach {next_attach_id}: {file_name}"
                )
                next_attach_id += 1

        page += 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FAISS ì¸ë±ìŠ¤ ë¹Œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_faiss_index() -> None:
    global vector_store
    if not parsed_notices:
        logger.warning("ë¹Œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    texts: List[str] = []
    metadatas: List[dict] = []

    for notice in parsed_notices.values():
        for attach in notice["attachments"]:
            raw_text = attach.content_html
            for chunk in splitter.split_text(raw_text):
                texts.append(chunk)
                metadatas.append({
                    "notice_title": notice["title"],
                    "attachment_id": attach.id,
                    "file_name": attach.file_name,
                })

    if not texts:
        logger.warning("ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    vector_store = FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas)
    vector_store.save_local(str(VECTOR_DIR))
    logger.info(f"[âœ… FAISS] {len(texts)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API ì—”ë“œí¬ì¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/notices")
def list_notices():
    return [
        {
            "notice_id": nid,
            "title": notice["title"],
            "attachments": [
                {"id": att.id, "file_name": att.file_name}
                for att in notice["attachments"]
            ],
        }
        for nid, notice in parsed_notices.items()
    ]


@app.get("/notices/{notice_id}/{attach_id}")
def get_attachment(notice_id: int, attach_id: int):
    notice = parsed_notices.get(notice_id)
    if not notice:
        raise HTTPException(404, "Notice not found")

    attach = next((a for a in notice["attachments"] if a.id == attach_id), None)
    if not attach:
        raise HTTPException(404, "Attachment not found")

    return {
        "notice_id": notice_id,
        "title": notice["title"],
        "attachment_id": attach_id,
        "file_name": attach.file_name,
        "content_html": attach.content_html,
        "content_text": attach.content_text,
    }


@app.post("/notices/refresh")
def refresh_notices(keyword: str = KEYWORD):
    parsed_notices.clear()
    global next_notice_id, next_attach_id
    next_notice_id = 1
    next_attach_id = 1
    try:
        crawl_and_parse(keyword)
        build_faiss_index()
        return {
            "status": "success",
            "notices": len(parsed_notices),
            "faiss": "built" if vector_store else "none",
        }
    except Exception as e:
        raise HTTPException(500, str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (ì˜µì…˜) ì„œë²„ ê¸°ë™ ì‹œ ìë™ ìˆ˜ì§‘ â”€â”€â”€
# crawl_and_parse()
