from __future__ import annotations
"""
PNU Scholarship Parser API â€“ v1.1
--------------------------------
* ê°œì„  ì‚¬í•­
  1. **ì¤‘ë³µ íŒŒì‹± ë°©ì§€** â€“ ì²¨ë¶€íŒŒì¼ì„ SHAâ€‘256 í•´ì‹œë¡œ ì‹ë³„í•´ ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œëŠ” ê±´ë„ˆëœ€.
  2. **FAISS ì¸ë±ìŠ¤ ì˜ì†/ì¦ë¶„** â€“ ì¸ë±ìŠ¤ ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ë©´ ë¡œë“œ í›„ ìƒˆ ì²­í¬ë§Œ ì¶”ê°€.
  3. **ì¥í•™ê¸ˆ ì¡°ê±´ í‚¤ì›Œë“œ ë§¤ì¹­** â€“ ê°„ë‹¨í•œ ë£° ê¸°ë°˜ ìŠ¤ìºë‹(ì•ŒëŒìš©) + ë¡œê¹….
  4. **ë¡œê¹… ê°•í™”** â€“ ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™©ê³¼ ìŠ¤í‚µ ì‚¬ìœ ê°€ ì½˜ì†”ì— ì¶œë ¥.

ì‹¤í–‰ ì˜ˆì‹œ
$ uvicorn scholarship_parser:app --reload
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€
import hashlib
import json
import logging
import mimetypes
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List
from urllib.parse import urljoin

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, BackgroundTasks
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_upstage import UpstageEmbeddings
from langchain_community.vectorstores import FAISS
from requests.adapters import HTTPAdapter
from requests.exceptions import HTTPError, ReadTimeout
from urllib3.util import Retry

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
if not UPSTAGE_API_KEY:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ UPSTAGE_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¡œê¹… â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger("pnu_parser")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter("%(levelname)s | %(message)s"))
logger.addHandler(handler)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(title="PNU Scholarship Parser API v1.1")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://cse.pusan.ac.kr"
LIST_URL = f"{BASE_URL}/bbs/cse/2605/artclList.do"
SUPPORTED_EXTENSIONS = (".pdf", ".hwp", ".hwpx", ".docx", ".ppt", ".pptx")
KEYWORD = "ì¥í•™"
CATEGORY_SEQ = "4229"
MAX_PAGES = 1
MAX_NOTICES = 8  # ì²˜ë¦¬í•  ê³µì§€ ê°œìˆ˜ í™•ëŒ€

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¥í•™ê¸ˆ ë£° (ì˜ˆì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCHOLARSHIP_RULES = {
    "êµ­ê°€ê·¼ë¡œ": ["êµ­ê°€ê·¼ë¡œ", "ê·¼ë¡œì¥í•™"],
    "í‘¸ë¥¸ë“±ëŒ€": ["í‘¸ë¥¸ë“±ëŒ€"],
    "ì§€ì—­ì¸ì¬": ["ì§€ì—­ì¸ì¬", "íŠ¹ë³„"]
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° êµ¬ì¡° â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class AttachmentDoc:
    id: int
    file_name: str
    content_html: str
    content_text: str
    matched_rules: List[str]

parsed_notices: Dict[int, Dict] = {}
next_notice_id: int = 1
next_attach_id: int = 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Embedding & FAISS â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings = UpstageEmbeddings(
    model="solar-embedding-1-large",
    upstage_api_key=UPSTAGE_API_KEY,
)
vector_store: FAISS | None = None
VECTOR_DIR = Path("./faiss_index")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•´ì‹œ ì €ì¥ì†Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
HASH_FILE = Path("known_hashes.json")
if HASH_FILE.exists():
    known_hashes = set(json.loads(HASH_FILE.read_text()))
else:
    known_hashes: set[str] = set()

def save_known_hashes():
    HASH_FILE.write_text(json.dumps(list(known_hashes), ensure_ascii=False, indent=2))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Session & Retry â”€â”€â”€â”€â”€â”€â”€â”€â”€
retry_policy = Retry(total=2, backoff_factor=1.5, allowed_methods={"POST"}, status_forcelist=[502, 503, 504])
session = requests.Session()
session.mount("https://", HTTPAdapter(max_retries=retry_policy))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sha256_bytes(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()


def guess_mime(fname: str) -> str:
    mime, _ = mimetypes.guess_type(fname)
    if mime:
        return mime
    return {".hwp": "application/x-hwp", ".hwpx": "application/x-hwp"}.get(Path(fname).suffix.lower(), "application/octet-stream")


def call_upstage(file_name: str, file_bytes: bytes) -> dict:
    """Upstage Document-Parse í˜¸ì¶œ í›„ JSON ë°˜í™˜"""
    api_url = "https://api.upstage.ai/v1/document-digitization"
    headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}
    files = {"document": (file_name, file_bytes, guess_mime(file_name))}
    data = {"model": "document-parse"}

    try:
        resp = session.post(api_url, headers=headers, files=files, data=data, timeout=(10, 180))
        resp.raise_for_status()
        return resp.json()
    except ReadTimeout:
        logger.warning(f"â° ReadTimeout â–¶ {file_name}")
        raise
    except HTTPError as e:
        logger.error(f"HTTPError â–¶ {file_name} â€“ {e}")
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‚¤ì›Œë“œ ë§¤ì¹­ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def match_rules(text: str) -> List[str]:
    matched = []
    lower = text.lower()
    for rule_name, kws in SCHOLARSHIP_RULES.items():
        if all(kw.lower() in lower for kw in kws):
            matched.append(rule_name)
    return matched

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€

def crawl_and_parse(keyword: str = KEYWORD, max_pages: int = MAX_PAGES, max_notices: int = MAX_NOTICES) -> None:
    """ì¥í•™ ì¹´í…Œê³ ë¦¬ + ì œëª© ê²€ìƒ‰ í¬ë¡¤ëŸ¬"""
    global next_notice_id, next_attach_id

    headers = {"User-Agent": "Mozilla/5.0"}
    logger.info("[START] í¬ë¡¤ë§ ì‹œì‘")

    page = 1
    while page <= max_pages and len(parsed_notices) < max_notices:
        payload = {"srchColumn": "sj", "srchWrd": keyword, "bbsClSeq": CATEGORY_SEQ, "page": str(page), "isViewMine": "false"}
        resp = session.post(LIST_URL, headers=headers, data=payload, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        articles = soup.select("td._artclTdTitle a.artclLinkView")
        if not articles:
            break
        logger.info(f"[INFO] page {page} - ê¸€ {len(articles)}ê°œ")

        for a in articles:
            if len(parsed_notices) >= max_notices:
                break

            row = a.find_parent("tr")
            if row and any(cls.startswith("headline") for cls in (row.get("class") or [])):
                logger.info(f"[SKIP] ê³ ì •ê³µì§€: {a.get_text(strip=True)}")
                continue

            notice_title = a.get_text(strip=True)
            detail_url = urljoin(BASE_URL, a["href"])
            logger.info(f"[INFO] ì²˜ë¦¬ ì¤‘(ê³µì§€): {notice_title}")

            notice_id = next_notice_id
            parsed_notices[notice_id] = {"title": notice_title, "attachments": []}
            next_notice_id += 1

            try:
                detail_resp = session.get(detail_url, headers=headers, timeout=15)
                detail_resp.raise_for_status()
                detail_soup = BeautifulSoup(detail_resp.text, "html.parser")
            except Exception as e:
                logger.error(f"[ERROR] ìƒì„¸ í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                continue

            attachments = detail_soup.select('dl.artclForm dd.artclInsert li a[href*="/download.do"]')
            if not attachments:
                logger.info("[SKIP] ì²¨ë¶€ ì—†ìŒ")
                continue

            for file_link in attachments:
                file_name = file_link.get_text(strip=True)
                if not file_name.lower().endswith(SUPPORTED_EXTENSIONS):
                    logger.warning(f"[SKIP] ë¯¸ì§€ì› í¬ë§·: {file_name}")
                    continue

                file_url = urljoin(detail_url, file_link["href"])
                try:
                    file_resp = session.get(file_url, headers=headers, timeout=30)
                    file_resp.raise_for_status()
                except Exception as e:
                    logger.error(f"[ERROR] íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_name} â€“ {e}")
                    continue

                f_hash = sha256_bytes(file_resp.content)
                if f_hash in known_hashes:
                    logger.info(f"[SKIP] ì¤‘ë³µ íŒŒì¼: {file_name}")
                    continue

                # â”€â”€ Upstage ë³€í™˜ ìš”ì²­ â”€â”€
                logger.info(f"ğŸ“¤ Upstage ìš”ì²­ ì‹œì‘ â–¶ {file_name}")
                try:
                    result_json = call_upstage(file_name, file_resp.content)
                except (ReadTimeout, HTTPError):
                    continue

                html_segments = [elem["content"]["html"] for elem in result_json.get("elements", []) if "content" in elem and "html" in elem["content"]]
                full_html = "\n".join(html_segments) or "<p>(ë¹ˆ ë¬¸ì„œ)</p>"

                text_segments = [BeautifulSoup(h, "html.parser").get_text(" ", strip=True) for h in html_segments]
                full_text = "\n".join(filter(None, text_segments)) or "(ë¹ˆ ë¬¸ì„œ)"

                matched = match_rules(full_text)
                if matched:
                    logger.info(f"ğŸ”” ì•Œë¦¼ ë§¤ì¹­: {matched} â€“ {file_name}")
                else:
                    logger.info(f"âšª ì¡°ê±´ ë¯¸ì¼ì¹˜: {file_name}")

                parsed_notices[notice_id]["attachments"].append(
                    AttachmentDoc(id=next_attach_id, file_name=file_name, content_html=full_html, content_text=full_text, matched_rules=matched)
                )
                next_attach_id += 1
                known_hashes.add(f_hash)
        page += 1

    save_known_hashes()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ FAISS ì¸ë±ì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_faiss_index() -> None:
    """ìƒˆ ë¬¸ì„œë§Œ ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ê±°ë‚˜, ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±"""
    global vector_store

    if not parsed_notices:
        logger.warning("ë¹Œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)

    # 1ï¸âƒ£ ìƒˆë¡œ ìˆ˜ì§‘ëœ ëª¨ë“  í…ìŠ¤íŠ¸ ì²­í¬ ìˆ˜ì§‘
    new_texts: List[str] = []
    new_metas: List[dict] = []

    for notice in parsed_notices.values():
        for attach in notice["attachments"]:
            for chunk in splitter.split_text(attach.content_html):
                new_texts.append(chunk)
                new_metas.append({
                    "notice_title": notice["title"],
                    "attachment_id": attach.id,
                    "file_name": attach.file_name,
                })

    if not new_texts:
        logger.info("[FAISS] ì¶”ê°€í•  ì²­í¬ ì—†ìŒ â€“ ì €ì¥ ìŠ¤í‚µ")
        return

    # 2ï¸âƒ£ ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ or ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
    if VECTOR_DIR.exists():
        logger.info("[FAISS] ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ")
        vector_store = FAISS.load_local(str(VECTOR_DIR), embeddings, allow_dangerous_deserialization=True,)
        vector_store.add_texts(new_texts, metadatas=new_metas)
    else:
        logger.info("[FAISS] ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")
        vector_store = FAISS.from_texts(texts=new_texts, embedding=embeddings, metadatas=new_metas)

    # 3ï¸âƒ£ ì €ì¥
    vector_store.save_local(str(VECTOR_DIR))
    logger.info(f"[âœ… FAISS] {len(new_texts)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ API â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/notices")
def list_notices():
    return [{"notice_id": nid, "title": n["title"], "attachments": [{"id": a.id, "file_name": a.file_name, "rules": a.matched_rules} for a in n["attachments"]]} for nid, n in parsed_notices.items()]


@app.get("/notices/{notice_id}/{attach_id}")
def get_attachment(notice_id: int, attach_id: int):
    notice = parsed_notices.get(notice_id)
    if not notice:
        raise HTTPException(404, "Notice not found")
    attach = next((a for a in notice["attachments"] if a.id == attach_id), None)
    if not attach:
        raise HTTPException(404, "Attachment not found")
    return {
        "notice_id": notice_id,
        "title": notice["title"],
        "attachment_id": attach_id,
        "file_name": attach.file_name,
        "matched_rules": attach.matched_rules,
        "content_html": attach.content_html,
        "content_text": attach.content_text,
    }


@app.post("/notices/refresh")
def refresh_notices(background_tasks: BackgroundTasks, keyword: str = KEYWORD):
    parsed_notices.clear()
    global next_notice_id, next_attach_id
    next_notice_id = 1
    next_attach_id = 1
    try:
        crawl_and_parse(keyword)
        # ì¸ë±ì‹±ì€ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰í•´ ì‘ë‹µ ì†ë„ í–¥ìƒ
        background_tasks.add_task(build_faiss_index)
        return {"status": "success", "notices": len(parsed_notices)}
    except Exception as e:
        raise HTTPException(500, str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ (ì˜µì…˜) ì„œë²„ ê¸°ë™ ì‹œ ìë™ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€
# crawl_and_parse()
